{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPe18iu4k7Uaa1GqGf704dr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Polt3r/PLN/blob/main/Aula_13_Redes_Neurais_PLN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resumo da Aula 14 – Redes Neurais para PLN\n",
        "\n",
        "Implementação de duas arquiteturas de redes neurais para resolver problemas específicos de PLN (Processamento de Linguagem Natural).\n",
        "\n",
        "O notebook implementa e demonstra duas aplicações básicas de redes neurais em PLN: previsão da próxima palavra com uma Simple RNN e classificação de sentimentos com uma LSTM, cobrindo desde a preparação dos dados até a construção, treinamento, avaliação e uso dos modelos.\n",
        "\n",
        "\n",
        "\n",
        "Utilização de técnicas e ferramentas para:\n",
        "\n",
        ">- **RNN Simples para Previsão de Próxima Palavra:**\n",
        "  - Preparação e pré-processamento de dados textuais\n",
        "  - Criação de sequências de entrada e targets\n",
        "  - Tokenização e conversão para representações numéricas\n",
        "  - Implementação de RNN com TensorFlow/Keras\n",
        "  - Treinamento do modelo para previsão sequencial\n",
        "  - Avaliação e geração de texto\n",
        "\n",
        "---\n",
        "\n",
        ">- **LSTM para Classificação de Sentimentos:**\n",
        "  - Preparação de dataset rotulado (positivo/negativo)\n",
        "  - Pré-processamento específico para classificação\n",
        "  - Implementação de arquitetura LSTM\n",
        "  - Treinamento supervisionado para classificação binária\n",
        "  - Avaliação com métricas de classificação\n",
        "  - Comparação de desempenho com modelos tradicionais\n",
        "\n",
        "---\n",
        "\n",
        ">- **Conceitos Fundamentais:**\n",
        "  - Diferenças entre RNN e LSTM\n",
        "  - Problema do desvanecimento do gradiente\n",
        "  - Arquiteturas sequenciais vs. classificação\n",
        "  - Embeddings de palavras e representações densas\n",
        "  - Técnicas de regularização (Dropout, Early Stopping)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "MfpPzCyLs2BM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necessárias\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "print(\"Bibliotecas importadas com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qJEFpNNSVWR",
        "outputId": "ece09d85-1411-4cdf-b9df-a57764b9aecc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicação das Bibliotecas:\n",
        "\n",
        "numpy: Fundamental para manipulação de arrays multidimensionais e operações matemáticas eficientes\n",
        "tensorflow.keras: Framework de alto nível para desenvolvimento e treinamento de modelos de aprendizado profundo\n",
        "Embedding: Camada responsável por converter índices de palavras em representações vetoriais densas\n",
        "SimpleRNN: Implementação básica de rede neural recorrente para processamento sequencial\n",
        "Dense: Camada totalmente conectada para transformações lineares e não-lineares\n",
        "Tokenizer: Ferramenta para transformar texto em sequências numéricas baseadas em vocabulário\n",
        "pad_sequences: Função para padronizar o tamanho das sequências, garantindo entradas uniformes para o modelo\n",
        "Tentar novamenteO Claude pode cometer erros. Confira sempre as respostas."
      ],
      "metadata": {
        "id": "ozex1lvkSWDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 2: Preparação do Conjunto de Dados\n",
        "\n",
        "# Conjunto de dados de treinamento (pequeno e simplificado)\n",
        "textos_treinamento = [\n",
        "    \"eu gosto de programar em python\",\n",
        "    \"python é uma linguagem poderosa\",\n",
        "    \"programar é divertido com python\",\n",
        "    \"adoro resolver problemas com código\",\n",
        "    \"gosto de aprender coisas novas\"\n",
        "]\n",
        "print(f\"Textos de treinamento: {textos_treinamento}\")\n",
        "\n",
        "# Inicializar o Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "# Construir o vocabulário a partir dos textos\n",
        "tokenizer.fit_on_texts(textos_treinamento)\n",
        "\n",
        "# Converter textos em sequências de números\n",
        "sequencias = tokenizer.texts_to_sequences(textos_treinamento)\n",
        "\n",
        "# Imprimir o vocabulário e as sequências geradas\n",
        "print(f\"\\nVocabulário (palavra: índice): {tokenizer.word_index}\")\n",
        "print(f\"Sequências numéricas dos textos: {sequencias}\")\n",
        "\n",
        "# Determinar o comprimento máximo das sequências (para o padding)\n",
        "total_palavras = len(tokenizer.word_index) + 1\n",
        "print(f\"Tamanho total do vocabulário: {total_palavras}\")\n",
        "\n",
        "# Preparar entradas (X) e saídas (y) para a previsão da próxima palavra\n",
        "# A entrada (X) será uma sequência de palavras, e a saída (y) será a palavra seguinte.\n",
        "# Determinar o comprimento máximo das sequências para padding\n",
        "max_comprimento = max(len(seq) for seq in sequencias)\n",
        "print(f\"Comprimento máximo das sequências antes do padding: {max_comprimento}\")\n",
        "\n",
        "# Criar pares de entrada (sequência parcial) e saída (próxima palavra)\n",
        "# Ex: \"eu gosto de programar\" -> \"em\"\n",
        "#     \"gosto de programar em\" -> \"python\"\n",
        "entradas_X = []\n",
        "saidas_y = []\n",
        "\n",
        "for seq in sequencias:\n",
        "    for i in range(1, len(seq)):\n",
        "        entradas_X.append(seq[:i])  # A sequência até a palavra atual\n",
        "        saidas_y.append(seq[i])     # A próxima palavra\n",
        "\n",
        "print(f\"Exemplo de entradas_X (parcial): {entradas_X[:5]}\")\n",
        "print(f\"Exemplo de saidas_y (parcial): {saidas_y[:5]}\")\n",
        "\n",
        "# Padronizar o comprimento das sequências de entrada\n",
        "# Usar pad_sequences para garantir que todas as sequências tenham o mesmo comprimento e RNN\n",
        "entradas_X_padded = pad_sequences(entradas_X, maxlen=max_comprimento - 1, padding='pre')\n",
        "# O maxlen é 'max_comprimento - 1' porque a saída 'y' é a última palavra, então X terá 1 palavra a menos.\n",
        "\n",
        "# Converter as saídas para o formato one-hot encoding\n",
        "# Isso é necessário para a camada de saída da RNN (softmax)\n",
        "saidas_y_one_hot = tf.keras.utils.to_categorical(saidas_y, num_classes=total_palavras)\n",
        "\n",
        "print(f\"\\nExemplo de entradas_X_padded (após padding e truncagem): {entradas_X_padded[:5]}\")\n",
        "print(f\"Exemplo de saidas_y_one_hot (one-hot encoding): {saidas_y_one_hot[:5]}\")\n",
        "print(f\"Formato final das entradas (X): {entradas_X_padded.shape}\")\n",
        "print(f\"Formato final das saídas (y): {saidas_y_one_hot.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYbbTIILSYn_",
        "outputId": "f92bcf05-e5ec-4ad0-bd91-dd1e6bbd7b7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textos de treinamento: ['eu gosto de programar em python', 'python é uma linguagem poderosa', 'programar é divertido com python', 'adoro resolver problemas com código', 'gosto de aprender coisas novas']\n",
            "\n",
            "Vocabulário (palavra: índice): {'python': 1, 'gosto': 2, 'de': 3, 'programar': 4, 'é': 5, 'com': 6, 'eu': 7, 'em': 8, 'uma': 9, 'linguagem': 10, 'poderosa': 11, 'divertido': 12, 'adoro': 13, 'resolver': 14, 'problemas': 15, 'código': 16, 'aprender': 17, 'coisas': 18, 'novas': 19}\n",
            "Sequências numéricas dos textos: [[7, 2, 3, 4, 8, 1], [1, 5, 9, 10, 11], [4, 5, 12, 6, 1], [13, 14, 15, 6, 16], [2, 3, 17, 18, 19]]\n",
            "Tamanho total do vocabulário: 20\n",
            "Comprimento máximo das sequências antes do padding: 6\n",
            "Exemplo de entradas_X (parcial): [[7], [7, 2], [7, 2, 3], [7, 2, 3, 4], [7, 2, 3, 4, 8]]\n",
            "Exemplo de saidas_y (parcial): [2, 3, 4, 8, 1]\n",
            "\n",
            "Exemplo de entradas_X_padded (após padding e truncagem): [[0 0 0 0 7]\n",
            " [0 0 0 7 2]\n",
            " [0 0 7 2 3]\n",
            " [0 7 2 3 4]\n",
            " [7 2 3 4 8]]\n",
            "Exemplo de saidas_y_one_hot (one-hot encoding): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Formato final das entradas (X): (21, 5)\n",
            "Formato final das saídas (y): (21, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloco foca na preparação de um pequeno conjunto de dados textuais para treinar uma Simple RNN na previsão da próxima palavra. Ele tokeniza o texto, cria sequências numéricas, determina o tamanho do vocabulário e prepara os pares de entrada (sequência parcial) e saída (próxima palavra) no formato adequado para a RNN, incluindo padding e one-hot encoding."
      ],
      "metadata": {
        "id": "7jN3USwLetZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 3: Construção do Modelo RNN\n",
        "\n",
        "# 1. Definindo o modelo\n",
        "# Definir a arquitetura do modelo RNN\n",
        "modelo_rnn = Sequential()\n",
        "\n",
        "# Camada de Embedding:\n",
        "# total_palavras: tamanho do vocabulário\n",
        "# 10: dimensão do vetor de embedding (quantas características queremos para cada palavra)\n",
        "# input_length: comprimento das sequências de entrada (maxlen - 1)\n",
        "modelo_rnn.add(Embedding(total_palavras, 10, input_length=entradas_X_padded.shape[1]))\n",
        "\n",
        "# Camada SimpleRNN:\n",
        "# 32: número de unidades (neurônios) na camada recorrente. Este é o tamanho do estado oculto.\n",
        "modelo_rnn.add(SimpleRNN(32))\n",
        "\n",
        "# Camada Densa de Saída:\n",
        "# total_palavras: número de neurônios de saída (um para cada palavra no vocabulário)\n",
        "# activation='softmax': função de ativação para probabilidade (como 1 para todas as palavras)\n",
        "modelo_rnn.add(Dense(total_palavras, activation='softmax'))\n",
        "\n",
        "# Compilar o modelo\n",
        "modelo_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Exibir um resumo da arquitetura do modelo\n",
        "modelo_rnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "g_HkKFX1Uign",
        "outputId": "8324e274-75a1-4c69-9a04-aedbb87991e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)          │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)          │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloco foca na preparação de um pequeno conjunto de dados textuais para treinar uma Simple RNN na previsão da próxima palavra. Ele tokeniza o texto, cria sequências numéricas, determina o tamanho do vocabulário e prepara os pares de entrada (sequência parcial) e saída (próxima palavra) no formato adequado para a RNN, incluindo padding e one-hot encoding."
      ],
      "metadata": {
        "id": "yOct17w5ev5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 4: Treinamento do Modelo\n",
        "\n",
        "# Treinando o modelo\n",
        "print(\"\\nIniciando o treinamento do modelo RNN...\")\n",
        "modelo_rnn.fit(entradas_X_padded, saidas_y_one_hot, epochs=100, verbose=1)\n",
        "#     epochs: quantas vezes o modelo verá todo o conjunto de dados\n",
        "#     verbose: 1 para mostrar o progresso do treinamento\n",
        "print(\"Treinamento concluído!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQbA_UGvUkIk",
        "outputId": "32b1a431-7f05-4ce5-dee8-8676339b4ac1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento do modelo RNN...\n",
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.0476 - loss: 2.9900\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.0952 - loss: 2.9819\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1429 - loss: 2.9738\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1429 - loss: 2.9656\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1429 - loss: 2.9572\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1429 - loss: 2.9487\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1429 - loss: 2.9400\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1429 - loss: 2.9310\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1429 - loss: 2.9218\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1429 - loss: 2.9122\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1429 - loss: 2.9023\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1429 - loss: 2.8921\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.1429 - loss: 2.8815\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1429 - loss: 2.8706\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.1429 - loss: 2.8592\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.1429 - loss: 2.8475\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1429 - loss: 2.8353\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1429 - loss: 2.8227\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1429 - loss: 2.8096\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1429 - loss: 2.7961\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1429 - loss: 2.7822\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.1429 - loss: 2.7678\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2381 - loss: 2.7530\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2381 - loss: 2.7377\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2381 - loss: 2.7220\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.2381 - loss: 2.7057\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2381 - loss: 2.6890\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1905 - loss: 2.6718\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.2381 - loss: 2.6541\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2381 - loss: 2.6360\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2381 - loss: 2.6174\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2857 - loss: 2.5984\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3333 - loss: 2.5790\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2381 - loss: 2.5593\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.2381 - loss: 2.5392\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2381 - loss: 2.5188\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2381 - loss: 2.4981\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2857 - loss: 2.4770\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2857 - loss: 2.4557\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2857 - loss: 2.4340\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2857 - loss: 2.4121\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.2857 - loss: 2.3898\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2857 - loss: 2.3673\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2857 - loss: 2.3446\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2857 - loss: 2.3216\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3333 - loss: 2.2985\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.3333 - loss: 2.2753\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.3810 - loss: 2.2520\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.3810 - loss: 2.2287\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3810 - loss: 2.2053\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3810 - loss: 2.1820\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.4286 - loss: 2.1588\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3810 - loss: 2.1357\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3810 - loss: 2.1129\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4286 - loss: 2.0902\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4286 - loss: 2.0677\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4286 - loss: 2.0455\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4286 - loss: 2.0236\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4286 - loss: 2.0018\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.4286 - loss: 1.9803\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4286 - loss: 1.9591\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4286 - loss: 1.9380\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4286 - loss: 1.9172\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4762 - loss: 1.8965\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4762 - loss: 1.8761\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5238 - loss: 1.8558\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4762 - loss: 1.8358\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4762 - loss: 1.8159\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 0.4762 - loss: 1.7961\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4762 - loss: 1.7766\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4762 - loss: 1.7572\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4762 - loss: 1.7380\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4762 - loss: 1.7189\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.4762 - loss: 1.7000\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.4762 - loss: 1.6812\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.4762 - loss: 1.6625\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.4762 - loss: 1.6439\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.5238 - loss: 1.6255\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.5238 - loss: 1.6072\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - accuracy: 0.5238 - loss: 1.5889\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5238 - loss: 1.5708\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.5238 - loss: 1.5528\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.5714 - loss: 1.5348\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.6190 - loss: 1.5170\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.6190 - loss: 1.4992\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.6667 - loss: 1.4816\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6667 - loss: 1.4640\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.6667 - loss: 1.4466\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.6667 - loss: 1.4292\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.6667 - loss: 1.4120\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6667 - loss: 1.3948\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6667 - loss: 1.3778\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6667 - loss: 1.3609\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6667 - loss: 1.3441\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6667 - loss: 1.3275\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6667 - loss: 1.3110\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6667 - loss: 1.2946\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6667 - loss: 1.2784\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6667 - loss: 1.2623\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.6667 - loss: 1.2464\n",
            "Treinamento concluído!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloco executa o treinamento do modelo Simple RNN usando os dados preparados. Ele especifica o número de épocas (quantas vezes o modelo revisa os dados) para o processo de aprendizado."
      ],
      "metadata": {
        "id": "TYjFmYTHeyik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 5: Usar o Modelo para Previsão\n",
        "\n",
        "# 1. Função de Previsão:\n",
        "def prever_proxima_palavra(modelo, tokenizer, max_seq_len, texto_base):\n",
        "    \"\"\"\n",
        "    Preve a próxima palavra dado um texto base.\n",
        "    \"\"\"\n",
        "    # Converter o texto base para sequência numérica\n",
        "    sequencia_numerica = tokenizer.texts_to_sequences([texto_base])[0]\n",
        "\n",
        "    # Aplicar o comprimento da sequência de entrada (precisa ter o mesmo formato que o treinamento)\n",
        "    # Atenção: max_seq_len deve ser o comprimento que as \"entradas\" foram pad_sequences\n",
        "    sequencia_padded = pad_sequences([sequencia_numerica], maxlen=max_seq_len, padding='pre')\n",
        "\n",
        "    # Fazer a previsão\n",
        "    previsao_probabilidades = modelo.predict(sequencia_padded, verbose=0)[0]\n",
        "\n",
        "    # Obter o índice da palavra com a maior probabilidade\n",
        "    indice_palavra_prevista = np.argmax(previsao_probabilidades)\n",
        "\n",
        "    # Converter o índice de volta para a palavra\n",
        "    for palavra, indice in tokenizer.word_index.items():\n",
        "        if indice == indice_palavra_prevista:\n",
        "            return palavra\n",
        "\n",
        "    return None  # Caso a palavra não seja encontrada (improvável com o vocabulário ajustado)\n",
        "\n",
        "# Comprimento de entrada esperado pelo modelo\n",
        "max_seq_len = entradas_X_padded.shape[1]  # o modelo que usamos no pad_sequences para X\n",
        "print(f\"Comprimento máximo esperado para as entradas: {max_seq_len}\")\n",
        "\n",
        "# Testar o modelo com novas frases\n",
        "print(\"\\n--- Testando o Modelo RNN ---\")\n",
        "\n",
        "texto_teste_1 = \"eu gosto de\"\n",
        "proxima_1 = prever_proxima_palavra(modelo_rnn, tokenizer, max_seq_len, texto_teste_1)\n",
        "print(f\"Texto: '{texto_teste_1}' -> Próxima palavra prevista: '{proxima_1}'\")\n",
        "\n",
        "texto_teste_2 = \"python é uma\"\n",
        "proxima_2 = prever_proxima_palavra(modelo_rnn, tokenizer, max_seq_len, texto_teste_2)\n",
        "print(f\"Texto: '{texto_teste_2}' -> Próxima palavra prevista: '{proxima_2}'\")\n",
        "\n",
        "texto_teste_3 = \"programar é divertido\"\n",
        "proxima_3 = prever_proxima_palavra(modelo_rnn, tokenizer, max_seq_len, texto_teste_3)\n",
        "print(f\"Texto: '{texto_teste_3}' -> Próxima palavra prevista: '{proxima_3}'\")\n",
        "\n",
        "texto_teste_4 = \"aprenda python e\"\n",
        "proxima_4 = prever_proxima_palavra(modelo_rnn, tokenizer, max_seq_len, texto_teste_4)\n",
        "print(f\"Texto: '{texto_teste_4}' -> Próxima palavra prevista: '{proxima_4}'\")\n",
        "\n",
        "# Função para usar uma palavra nova não vista no treinamento (ou sequência que o modelo nunca viu antes)\n",
        "texto_teste_5 = \"o sol brilha no\" # Palavras \"sol\" e \"brilha\" não estão no vocabulário\n",
        "proxima_5 = prever_proxima_palavra(modelo_rnn, tokenizer, max_seq_len, texto_teste_5)\n",
        "print(f\"Texto: '{texto_teste_5}' -> Próxima palavra prevista: '{proxima_5}' (Note ser imprecisa devido às palavras desconhecidas)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOzWQwzZUrHU",
        "outputId": "eed092d6-c2c9-47d5-87a4-50e204455278"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comprimento máximo esperado para as entradas: 5\n",
            "\n",
            "--- Testando o Modelo RNN ---\n",
            "Texto: 'eu gosto de' -> Próxima palavra prevista: 'programar'\n",
            "Texto: 'python é uma' -> Próxima palavra prevista: 'com'\n",
            "Texto: 'programar é divertido' -> Próxima palavra prevista: 'com'\n",
            "Texto: 'aprenda python e' -> Próxima palavra prevista: 'é'\n",
            "Texto: 'o sol brilha no' -> Próxima palavra prevista: 'de' (Note ser imprecisa devido às palavras desconhecidas)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloco contém uma função para usar o modelo RNN treinado para prever a próxima palavra dado um texto base. Ele demonstra como usar a função com alguns exemplos, mostrando a palavra prevista para diferentes entradas."
      ],
      "metadata": {
        "id": "IMWkFTCte1gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar bibliotecas necessárias\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"Bibliotecas importadas com sucesso!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReF3_-XmVgFp",
        "outputId": "e93cfd50-45da-4a83-9a88-88a879d7b315"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bibliotecas importadas com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar ao primeiro bloco, mas importa bibliotecas específicas e adicionais para a tarefa de Classificação de Sentimentos usando LSTM, incluindo a camada LSTM, train_test_split para dividir dados, e classification_report, confusion_matrix, matplotlib e seaborn para avaliação e visualização de resultados."
      ],
      "metadata": {
        "id": "ZBbSPREZe4x5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir o conjunto de dados (frases e rótulos) para análise de sentimentos\n",
        "dados_sentimento = [\n",
        "    (\"este filme é ótimo e divertido\", \"positivo\"),\n",
        "    (\"não adorei o livro, muito baixo\", \"negativo\"),\n",
        "    (\"a experiência foi incrível e satisfatória\", \"positivo\"),\n",
        "    (\"o roteiro é fraco e chato\", \"negativo\"),\n",
        "    (\"não recomendo esta péssima produto\", \"negativo\"),\n",
        "    (\"adorei a nova versão do aplicativo\", \"positivo\"),\n",
        "    (\"ótimo trabalho, parabéns\", \"positivo\"),\n",
        "    (\"terrível experiência, nunca mais\", \"negativo\"),\n",
        "    (\"excelente serviço, muito eficiente\", \"positivo\"),\n",
        "    (\"que decepção, muito ruim\", \"negativo\"),\n",
        "    (\"fantástico, recomendo para todos\", \"positivo\"),\n",
        "    (\"blu é um campo interessante\", \"positivo\"),\n",
        "    (\"este software travou várias vezes\", \"negativo\"),\n",
        "    (\"perfeito, funciona como esperado\", \"positivo\"),\n",
        "    (\"o aplicativo é super útil e rápido\", \"positivo\"),\n",
        "]\n",
        "\n",
        "# Separar textos e sentimentos\n",
        "textos = [dado[0] for dado in dados_sentimento]\n",
        "sentimentos = [dado[1] for dado in dados_sentimento]\n",
        "\n",
        "print(f\"Total de frases: {len(textos)}\")\n",
        "print(f\"Exemplo de textos: {textos[:3]}\")\n",
        "print(f\"Exemplo de sentimentos: {sentimentos[:3]}\")\n",
        "\n",
        "# Mapear sentimentos para números (converter \"positivo\" e \"negativo\" para 0 e 1)\n",
        "mapeamento_sentimentos = {\"negativo\": 0, \"positivo\": 1}\n",
        "rotulos_numericos = [mapeamento_sentimentos[sentimento] for sentimento in sentimentos]\n",
        "\n",
        "print(f\"\\nSentimentos mapeados para números: {rotulos_numericos}\")\n",
        "\n",
        "# Tokenização de Texto\n",
        "tokenizer = Tokenizer(num_words=None, oov_token=\"<unk>\")\n",
        "# num_words=None para pegar todo o vocabulário\n",
        "# oov_token para palavras desconhecidas\n",
        "tokenizer.fit_on_texts(textos)\n",
        "sequencias_numericas = tokenizer.texts_to_sequences(textos)\n",
        "\n",
        "total_palavras_vocab = len(tokenizer.word_index) + 1  # +1 para o padding/UNK\n",
        "print(f\"\\nTotal de palavras únicas: {total_palavras_vocab}\")\n",
        "print(f\"Sequências numéricas das frases: {sequencias_numericas[:3]}\")\n",
        "print(f\"Tamanho total do vocabulário: {total_palavras_vocab}\")\n",
        "\n",
        "# Padronizar o comprimento das sequências\n",
        "max_comprimento = max(len(seq) for seq in sequencias_numericas)\n",
        "print(f\"\\nComprimento máximo das sequências: {max_comprimento}\")\n",
        "\n",
        "sequencias_padded = pad_sequences(sequencias_numericas, maxlen=max_comprimento, padding='post')  # 'post' para adicionar zeros no final\n",
        "print(f\"\\nSequências após padding: {sequencias_padded[:3]}\")\n",
        "\n",
        "# Dividir os dados em treino e teste\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(\n",
        "    sequencias_padded, rotulos_numericos, test_size=0.2, random_state=42, stratify=rotulos_numericos\n",
        ")\n",
        "\n",
        "# Converter as listas y_treino e y_teste para arrays NumPy\n",
        "y_treino = np.array(y_treino)\n",
        "y_teste = np.array(y_teste)\n",
        "\n",
        "\n",
        "print(f\"\\nShape de X_treino: {X_treino.shape}\")\n",
        "print(f\"Shape de X_teste: {X_teste.shape}\")\n",
        "print(f\"Shape de y_treino: {y_treino.shape}\")\n",
        "print(f\"Shape de y_teste: {y_teste.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6xpLQq3Vg1Y",
        "outputId": "f93e92ee-e288-4554-fc95-d2e1314673ca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de frases: 15\n",
            "Exemplo de textos: ['este filme é ótimo e divertido', 'não adorei o livro, muito baixo', 'a experiência foi incrível e satisfatória']\n",
            "Exemplo de sentimentos: ['positivo', 'negativo', 'positivo']\n",
            "\n",
            "Sentimentos mapeados para números: [1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1]\n",
            "\n",
            "Total de palavras únicas: 59\n",
            "Sequências numéricas das frases: [[6, 14, 2, 7, 3, 15], [8, 9, 4, 16, 5, 17], [10, 11, 18, 19, 3, 20]]\n",
            "Tamanho total do vocabulário: 59\n",
            "\n",
            "Comprimento máximo das sequências: 7\n",
            "\n",
            "Sequências após padding: [[ 6 14  2  7  3 15  0]\n",
            " [ 8  9  4 16  5 17  0]\n",
            " [10 11 18 19  3 20  0]]\n",
            "\n",
            "Shape de X_treino: (12, 7)\n",
            "Shape de X_teste: (3, 7)\n",
            "Shape de y_treino: (12,)\n",
            "Shape de y_teste: (3,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloco prepara um conjunto de dados de frases rotuladas (positivo/negativo) para a tarefa de classificação de sentimentos. Ele separa textos e rótulos, mapeia sentimentos para números, tokeniza as frases, padroniza o comprimento das sequências e divide os dados em conjuntos de treino e teste usando train_test_split."
      ],
      "metadata": {
        "id": "xIidMFmRe8jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir a arquitetura do modelo LSTM\n",
        "modelo_lstm = Sequential()\n",
        "\n",
        "# Camada de Embedding: converte índices numéricos das palavras em vetores densos.\n",
        "# total_palavras_vocab: tamanho do vocabulário\n",
        "# embedding_dim: 50 dimensão do vetor de embedding (pode ser ajustado)\n",
        "# input_length: comprimento padronizado das sequências (max_len)\n",
        "modelo_lstm.add(Embedding(total_palavras_vocab, 50, input_length=max_comprimento))\n",
        "\n",
        "# Camada LSTM:\n",
        "# 64: número de unidades (neurônios) na camada LSTM. Define o tamanho do estado oculto e da célula de memória.\n",
        "# dropout: um tipo de regularização para evitar overfitting (descarta aleatoriamente neurônios durante o treinamento).\n",
        "# recurrent_dropout: Dropout aplicado nas conexões recorrentes da LSTM.\n",
        "modelo_lstm.add(LSTM(64, dropout=0.5, recurrent_dropout=0.5))\n",
        "\n",
        "# Camada Densa de Saída:\n",
        "# 1: um único neurônio de saída, pois é um problema de classificação binária (positivo/negativo).\n",
        "# activation='sigmoid': função de ativação para classificação binária (produz um valor entre 0 e 1).\n",
        "modelo_lstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compilar o modelo\n",
        "modelo_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Exibir um resumo da arquitetura do modelo\n",
        "modelo_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "sLUtyR2cVnQe",
        "outputId": "952cf3b2-6b6a-4644-9e59-7ea6aee1095c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a arquitetura do modelo LSTM para classificação de sentimentos. Inclui uma camada de Embedding, uma camada LSTM (com dropout para regularização) e uma camada Dense de saída com ativação sigmoid para a classificação binária. O modelo é então compilado."
      ],
      "metadata": {
        "id": "YmlIkkAWfAvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar o modelo\n",
        "print(\"\\nIniciando o treinamento do modelo LSTM...\")\n",
        "historico = modelo_lstm.fit(\n",
        "    X_treino, y_treino,\n",
        "    epochs=20,  # Reduza para 20 epochs para um treinamento mais rápido no exemplo. Pode ser aumentado.\n",
        "    batch_size=2,  # Pequeno batch size para dataset pequeno.\n",
        "    validation_split=0.2,  # Usar 20% de treino para validação\n",
        "    verbose=1\n",
        ")\n",
        "# epochs: número de vezes que o modelo verá todo o conjunto de dados de treinamento.\n",
        "# batch_size: quantos exemplos o modelo processa por vez ao atualizar os pesos.\n",
        "# validation_split: % dos dados de treino usados para validação durante o treinamento (opcional, mas bem para monitorar overfitting).\n",
        "print(\"Treinamento concluído!\")\n",
        "\n",
        "# Avaliar o modelo no conjunto de teste\n",
        "perda, acuracia = modelo_lstm.evaluate(X_teste, y_teste, verbose=0)\n",
        "print(f\"\\nAcurácia do modelo no conjunto de teste: {acuracia:.2f}\")\n",
        "print(f\"Perda no conjunto de teste: {perda:.2f}\")\n",
        "\n",
        "# Fazer previsões no conjunto de teste\n",
        "y_pred_prob = modelo_lstm.predict(X_teste)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)  # Converter probabilidades para 0 ou 1\n",
        "\n",
        "print(\"\\n--- Relatório de Classificação ---\")\n",
        "print(classification_report(y_teste, y_pred, target_names=['negativo', 'positivo']))\n",
        "\n",
        "print(\"\\n--- Matriz de Confusão ---\")\n",
        "cm = confusion_matrix(y_teste, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negativo', 'positivo'], yticklabels=['negativo', 'positivo'])\n",
        "plt.xlabel('Previsão')\n",
        "plt.ylabel('Real')\n",
        "plt.title('Matriz de Confusão')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MlX8w9xlVs2w",
        "outputId": "61ba2820-7490-4f1b-b6f3-efebcace29e2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando o treinamento do modelo LSTM...\n",
            "Epoch 1/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 220ms/step - accuracy: 0.3542 - loss: 0.6954 - val_accuracy: 0.6667 - val_loss: 0.6937\n",
            "Epoch 2/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.6667 - loss: 0.6923 - val_accuracy: 0.6667 - val_loss: 0.6930\n",
            "Epoch 3/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7778 - loss: 0.6883 - val_accuracy: 0.6667 - val_loss: 0.6936\n",
            "Epoch 4/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.5787 - loss: 0.6900 - val_accuracy: 0.6667 - val_loss: 0.6924\n",
            "Epoch 5/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.4931 - loss: 0.6848 - val_accuracy: 0.6667 - val_loss: 0.6908\n",
            "Epoch 6/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.7778 - loss: 0.6714 - val_accuracy: 0.6667 - val_loss: 0.6876\n",
            "Epoch 7/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7292 - loss: 0.6580 - val_accuracy: 0.6667 - val_loss: 0.6858\n",
            "Epoch 8/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.7315 - loss: 0.6746 - val_accuracy: 0.6667 - val_loss: 0.6852\n",
            "Epoch 9/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9421 - loss: 0.6558 - val_accuracy: 0.6667 - val_loss: 0.6827\n",
            "Epoch 10/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 0.8727 - loss: 0.6272 - val_accuracy: 0.6667 - val_loss: 0.6782\n",
            "Epoch 11/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.5792 - val_accuracy: 0.6667 - val_loss: 0.6713\n",
            "Epoch 12/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.5155 - val_accuracy: 0.6667 - val_loss: 0.6570\n",
            "Epoch 13/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.4634 - val_accuracy: 0.6667 - val_loss: 0.6368\n",
            "Epoch 14/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.3587 - val_accuracy: 0.6667 - val_loss: 0.6044\n",
            "Epoch 15/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.2206 - val_accuracy: 0.6667 - val_loss: 0.5816\n",
            "Epoch 16/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0904 - val_accuracy: 0.6667 - val_loss: 0.5809\n",
            "Epoch 17/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - accuracy: 1.0000 - loss: 0.1339 - val_accuracy: 0.6667 - val_loss: 0.6032\n",
            "Epoch 18/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0652 - val_accuracy: 0.6667 - val_loss: 0.6274\n",
            "Epoch 19/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 1.0000 - loss: 0.0318 - val_accuracy: 0.6667 - val_loss: 0.6429\n",
            "Epoch 20/20\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0255 - val_accuracy: 0.6667 - val_loss: 0.6674\n",
            "Treinamento concluído!\n",
            "\n",
            "Acurácia do modelo no conjunto de teste: 0.33\n",
            "Perda no conjunto de teste: 0.87\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step\n",
            "\n",
            "--- Relatório de Classificação ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negativo       0.00      0.00      0.00         1\n",
            "    positivo       0.50      0.50      0.50         2\n",
            "\n",
            "    accuracy                           0.33         3\n",
            "   macro avg       0.25      0.25      0.25         3\n",
            "weighted avg       0.33      0.33      0.33         3\n",
            "\n",
            "\n",
            "--- Matriz de Confusão ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este bloco treina o modelo LSTM usando os dados de treino e avalia seu desempenho no conjunto de teste. Ele imprime a acurácia e a perda, e mostra um relatório de classificação e uma matriz de confusão para detalhar os resultados da classificação."
      ],
      "metadata": {
        "id": "wTd3pm5nfDqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 5: Testar o Modelo com Novas Frases\n",
        "\n",
        "# Utilizando o modelo treinado\n",
        "def prever_sentimento(modelo, tokenizer, max_seq_len, frase_nova, mapeamento_sentimentos):\n",
        "    \"\"\"\n",
        "    Prevê o sentimento de uma nova frase.\n",
        "    \"\"\"\n",
        "    # Converter a frase para sequência numérica\n",
        "    sequencia_numerica = tokenizer.texts_to_sequences([frase_nova])\n",
        "\n",
        "    # Se a frase tem palavras desconhecidas, o tokenizer pode retornar uma lista vazia ou valores 0\n",
        "    if not sequencia_numerica or not sequencia_numerica[0]:\n",
        "        print(f\"Aviso: a frase '{frase_nova}' contém apenas palavras desconhecidas.\")\n",
        "        return \"não é possível\" # ou outra indicação\n",
        "\n",
        "    sequencia_numerica = sequencia_numerica[0]  # Pega a primeira (e única) sequência\n",
        "\n",
        "    # Padronizar o comprimento da sequência de entrada\n",
        "    sequencia_padded = pad_sequences([sequencia_numerica], maxlen=max_comprimento, padding='post')\n",
        "\n",
        "    # Fazer a previsão (probabilidade)\n",
        "    probabilidade_positiva = modelo.predict(sequencia_padded, verbose=0)[0][0]\n",
        "\n",
        "    # Inverter o mapeamento para obter o nome do sentimento\n",
        "    mapeamento_inverso = {v: k for k, v in mapeamento_sentimentos.items()}\n",
        "\n",
        "    # Classificar com base no limiar de 0.5\n",
        "    if probabilidade_positiva >= 0.5:\n",
        "        return mapeamento_inverso[1] # 'positivo'\n",
        "    else:\n",
        "        return mapeamento_inverso[0] # 'negativo'\n",
        "\n",
        "# Testar o modelo com novas frases\n",
        "print(\"\\n--- Testando o Modelo LSTM com Novas Frases ---\")\n",
        "\n",
        "# Novas frases inventadas, mantendo o contexto de sentimento\n",
        "frase_nova_1 = \"adorei a performance, foi espetacular!\" # Positivo\n",
        "sentimento_1 = prever_sentimento(modelo_lstm, tokenizer, max_comprimento, frase_nova_1, mapeamento_sentimentos)\n",
        "print(f\"Frase: '{frase_nova_1}' -> Sentimento previsto: '{sentimento_1}'\")\n",
        "\n",
        "frase_nova_2 = \"que experiência frustrante, não funcionou\" # Negativo\n",
        "sentimento_2 = prever_sentimento(modelo_lstm, tokenizer, max_comprimento, frase_nova_2, mapeamento_sentimentos)\n",
        "print(f\"Frase: '{frase_nova_2}' -> Sentimento previsto: '{sentimento_2}'\")\n",
        "\n",
        "frase_nova_3 = \"o resultado foi surpreendente de bom\" # Positivo\n",
        "sentimento_3 = prever_sentimento(modelo_lstm, tokenizer, max_comprimento, frase_nova_3, mapeamento_sentimentos)\n",
        "print(f\"Frase: '{frase_nova_3}' -> Sentimento previsto: '{sentimento_3}'\")\n",
        "\n",
        "frase_nova_4 = \"este serviço é inaceitável de tão ruim\" # Negativo\n",
        "sentimento_4 = prever_sentimento(modelo_lstm, tokenizer, max_comprimento, frase_nova_4, mapeamento_sentimentos)\n",
        "print(f\"Frase: '{frase_nova_4}' -> Sentimento previsto: '{sentimento_4}'\")\n",
        "\n",
        "frase_nova_5 = \"tivemos um dia maravilhoso no parque\" # Positivo\n",
        "sentimento_5 = prever_sentimento(modelo_lstm, tokenizer, max_comprimento, frase_nova_5, mapeamento_sentimentos)\n",
        "print(f\"Frase: '{frase_nova_5}' -> Sentimento previsto: '{sentimento_5}'\")\n",
        "\n",
        "frase_nova_6 = \"a comida estava sem gosto e fria\" # Negativo\n",
        "sentimento_6 = prever_sentimento(modelo_lstm, tokenizer, max_comprimento, frase_nova_6, mapeamento_sentimentos)\n",
        "print(f\"Frase: '{frase_nova_6}' -> Sentimento previsto: '{sentimento_6}'\")\n",
        "\n",
        "frase_nova_7 = \"recomendo fortemente, vale cada centavo\" # Positivo\n",
        "sentimento_7 = prever_sentimento(modelo_lstm, tokenizer, max_comprimento, frase_nova_7, mapeamento_sentimentos)\n",
        "print(f\"Frase: '{frase_nova_7}' -> Sentimento previsto: '{sentimento_7}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2hURdQaV9J4",
        "outputId": "1a1eb557-dd7a-4c1f-a59e-de63b085453d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testando o Modelo LSTM com Novas Frases ---\n",
            "Frase: 'adorei a performance, foi espetacular!' -> Sentimento previsto: 'positivo'\n",
            "Frase: 'que experiência frustrante, não funcionou' -> Sentimento previsto: 'negativo'\n",
            "Frase: 'o resultado foi surpreendente de bom' -> Sentimento previsto: 'negativo'\n",
            "Frase: 'este serviço é inaceitável de tão ruim' -> Sentimento previsto: 'positivo'\n",
            "Frase: 'tivemos um dia maravilhoso no parque' -> Sentimento previsto: 'positivo'\n",
            "Frase: 'a comida estava sem gosto e fria' -> Sentimento previsto: 'positivo'\n",
            "Frase: 'recomendo fortemente, vale cada centavo' -> Sentimento previsto: 'positivo'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contém uma função para prever o sentimento de novas frases usando o modelo LSTM treinado. Ele demonstra como usar a função com vários exemplos de frases, mostrando o sentimento previsto (positivo ou negativo) para cada uma, com certa margem de erro, teste de reforço de treino."
      ],
      "metadata": {
        "id": "SY51_b4FfGkG"
      }
    }
  ]
}